{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8359de32",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Final Project: Exploration Strategies for Robotic Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982d8d8",
   "metadata": {},
   "source": [
    "### Import helper libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b9b0cc99-0fb3-41eb-8385-43664701355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important trick to keep JupyterLab from crashing\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import numpy as np\n",
    "import gym\n",
    "import sys\n",
    "from arguments import get_args\n",
    "import random\n",
    "import torch\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "from mpi4py import MPI\n",
    "from mpi_utils.mpi_utils import sync_networks, sync_grads\n",
    "from rl_modules.replay_buffer import replay_buffer\n",
    "from rl_modules.models import actor, critic\n",
    "from mpi_utils.normalizer import normalizer\n",
    "from her_modules.her import her_sampler\n",
    "import copy\n",
    "import math\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import more_itertools as mitt\n",
    "import pygame\n",
    "import glfw\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = [36, 4]\n",
    "fullPath = str(Path('.').absolute())\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['IN_MPI'] = '1'\n",
    "\n",
    "### Create environments and place to store model weights\n",
    "envs = {\n",
    "    'push': {\n",
    "        'model': gym.make('FetchPush-v1'),\n",
    "        'weights': None\n",
    "    },\n",
    "    'reach': {\n",
    "        'model': gym.make('FetchReach-v1'),\n",
    "        'weights': None\n",
    "    },\n",
    "    'slide': {\n",
    "        'model': gym.make('FetchSlide-v1'),\n",
    "        'weights': None\n",
    "    },\n",
    "    'pick': {\n",
    "        'model': gym.make('FetchPickAndPlace-v1'),\n",
    "        'weights': None\n",
    "    }\n",
    "}\n",
    "\n",
    "### List of Exploration Policies\n",
    "exploration_policies = {\n",
    "    'e-greedy': None,\n",
    "    'semi-uniform-distributed': None,\n",
    "    'boltzmann-distributed': None,\n",
    "    'UCB': None,\n",
    "    'counter-based': None,\n",
    "    'counter-based-decay': None,\n",
    "    'recency-based': None,\n",
    "    'OFU': None,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fb3597",
   "metadata": {},
   "source": [
    "### Graphically Render Policy\n",
    "\n",
    "Using Mujoco backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b6b08df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env, policy=None):\n",
    "    \"\"\"Graphically render an episode using the given policy\n",
    "\n",
    "    :param env:  Gym environment\n",
    "    :param policy:  function which maps state to action.  If None, the random\n",
    "                    policy is used.\n",
    "    \"\"\"\n",
    "    glfw.init()\n",
    "    if policy is None:\n",
    "\n",
    "        def policy(state):\n",
    "            return env.action_space.sample()\n",
    "\n",
    "    env = env['model'].env\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    while True:\n",
    "        action = policy(state)\n",
    "        state, _, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    env.close()\n",
    "    glfw.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b1f3b",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "baf50da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_og(o, g):\n",
    "    o = np.clip(o, -200, 200)\n",
    "    g = np.clip(g, -200, 200)\n",
    "    return o, g\n",
    "\n",
    "# pre_process the inputs\n",
    "def preproc_inputs(obs, g):\n",
    "    obs_norm = self.o_norm.normalize(obs)\n",
    "    g_norm = self.g_norm.normalize(g)\n",
    "    # concatenate the stuffs\n",
    "    inputs = np.concatenate([obs_norm, g_norm])\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c211164e",
   "metadata": {},
   "source": [
    "### Define Structure for Deep Deterministic Policy Gradient (DDPG) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d5496d8a-67e0-45af-9871-e859ecbca084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ddpg with HER (MPI-version)\n",
    "\n",
    "\"\"\"\n",
    "class ddpg_agent:\n",
    "    def __init__(self, args, env, env_params):\n",
    "        self.args = args\n",
    "        self.env = env\n",
    "        self.env_params = env_params\n",
    "        # create the network\n",
    "        self.actor_network = actor(env_params)\n",
    "        self.critic_network = critic(env_params)\n",
    "        # sync the networks across the cpus\n",
    "        sync_networks(self.actor_network)\n",
    "        sync_networks(self.critic_network)\n",
    "        # build up the target network\n",
    "        self.actor_target_network = actor(env_params)\n",
    "        self.critic_target_network = critic(env_params)\n",
    "        # load the weights into the target networks\n",
    "        self.actor_target_network.load_state_dict(self.actor_network.state_dict())\n",
    "        self.critic_target_network.load_state_dict(self.critic_network.state_dict())\n",
    "        # if use gpu\n",
    "        if self.args.cuda:\n",
    "            self.actor_network.cuda()\n",
    "            self.critic_network.cuda()\n",
    "            self.actor_target_network.cuda()\n",
    "            self.critic_target_network.cuda()\n",
    "        # create the optimizer\n",
    "        self.actor_optim = torch.optim.Adam(self.actor_network.parameters(), lr=self.args.lr_actor)\n",
    "        self.critic_optim = torch.optim.Adam(self.critic_network.parameters(), lr=self.args.lr_critic)\n",
    "        # her sampler\n",
    "        self.her_module = her_sampler(self.args.replay_strategy, self.args.replay_k, self.env.compute_reward)\n",
    "        # create the replay buffer\n",
    "        self.buffer = replay_buffer(self.env_params, self.args.buffer_size, self.her_module.sample_her_transitions)\n",
    "        # create the normalizer\n",
    "        self.o_norm = normalizer(size=env_params['obs'], default_clip_range=self.args.clip_range)\n",
    "        self.g_norm = normalizer(size=env_params['goal'], default_clip_range=self.args.clip_range)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        train the network\n",
    "\n",
    "        \"\"\"\n",
    "        # start to collect samples\n",
    "        pbar = tqdm.notebook.trange(self.args.n_epochs)\n",
    "        for epoch in pbar:\n",
    "            for _ in range(self.args.n_cycles):\n",
    "                mb_obs, mb_ag, mb_g, mb_actions = [], [], [], []\n",
    "                for _ in range(self.args.num_rollouts_per_mpi):\n",
    "                    # reset the rollouts\n",
    "                    ep_obs, ep_ag, ep_g, ep_actions = [], [], [], []\n",
    "                    # reset the environment\n",
    "                    observation = self.env.reset()\n",
    "                    obs = observation['observation']\n",
    "                    ag = observation['achieved_goal']\n",
    "                    g = observation['desired_goal']\n",
    "                    # start to collect samples\n",
    "                    for t in range(self.env_params['max_timesteps']):\n",
    "                        with torch.no_grad():\n",
    "                            input_tensor = self._preproc_inputs(obs, g)\n",
    "                            pi = self.actor_network(input_tensor)\n",
    "                            action = self._select_actions(pi)\n",
    "                        # feed the actions into the environment\n",
    "                        observation_new, _, _, info = self.env.step(action)\n",
    "                        obs_new = observation_new['observation']\n",
    "                        ag_new = observation_new['achieved_goal']\n",
    "                        # append rollouts\n",
    "                        ep_obs.append(obs.copy())\n",
    "                        ep_ag.append(ag.copy())\n",
    "                        ep_g.append(g.copy())\n",
    "                        ep_actions.append(action.copy())\n",
    "                        # re-assign the observation\n",
    "                        obs = obs_new\n",
    "                        ag = ag_new\n",
    "                    ep_obs.append(obs.copy())\n",
    "                    ep_ag.append(ag.copy())\n",
    "                    mb_obs.append(ep_obs)\n",
    "                    mb_ag.append(ep_ag)\n",
    "                    mb_g.append(ep_g)\n",
    "                    mb_actions.append(ep_actions)\n",
    "                # convert them into arrays\n",
    "                mb_obs = np.array(mb_obs)\n",
    "                mb_ag = np.array(mb_ag)\n",
    "                mb_g = np.array(mb_g)\n",
    "                mb_actions = np.array(mb_actions)\n",
    "                # store the episodes\n",
    "                self.buffer.store_episode([mb_obs, mb_ag, mb_g, mb_actions])\n",
    "                self._update_normalizer([mb_obs, mb_ag, mb_g, mb_actions])\n",
    "                for _ in range(self.args.n_batches):\n",
    "                    # train the network\n",
    "                    self._update_network()\n",
    "                # soft update\n",
    "                self._soft_update_target_network(self.actor_target_network, self.actor_network)\n",
    "                self._soft_update_target_network(self.critic_target_network, self.critic_network)\n",
    "            # start to do the evaluation\n",
    "            success_rate = self._eval_agent()\n",
    "            if MPI.COMM_WORLD.Get_rank() == 0:\n",
    "                print('[{}] epoch is: {}, eval success rate is: {:.3f}'.format(datetime.now(), epoch, success_rate))\n",
    "\n",
    "        # Return the network weights\n",
    "        #torch.save([self.o_norm.mean, self.o_norm.std, self.g_norm.mean, self.g_norm.std, self.actor_network.state_dict()], \\\n",
    "        #    self.model_path + '/model.pt')\n",
    "        return self.actor_network, self.o_norm, self.g_norm\n",
    "\n",
    "    # pre_process the inputs\n",
    "    def _preproc_inputs(self, obs, g):\n",
    "        obs_norm = self.o_norm.normalize(obs)\n",
    "        g_norm = self.g_norm.normalize(g)\n",
    "        # concatenate the stuffs\n",
    "        inputs = np.concatenate([obs_norm, g_norm])\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "        if self.args.cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        return inputs\n",
    "    \n",
    "    # this function will choose action for the agent and do the exploration\n",
    "    def _select_actions(self, pi):\n",
    "        action = pi.cpu().numpy().squeeze()\n",
    "        # add the gaussian\n",
    "        action += self.args.noise_eps * self.env_params['action_max'] * np.random.randn(*action.shape)\n",
    "        action = np.clip(action, -self.env_params['action_max'], self.env_params['action_max'])\n",
    "        # random actions...\n",
    "        random_actions = np.random.uniform(low=-self.env_params['action_max'], high=self.env_params['action_max'], \\\n",
    "                                            size=self.env_params['action'])\n",
    "        # choose if use the random actions\n",
    "        action += np.random.binomial(1, self.args.random_eps, 1)[0] * (random_actions - action)\n",
    "        return action\n",
    "\n",
    "    # update the normalizer\n",
    "    def _update_normalizer(self, episode_batch):\n",
    "        mb_obs, mb_ag, mb_g, mb_actions = episode_batch\n",
    "        mb_obs_next = mb_obs[:, 1:, :]\n",
    "        mb_ag_next = mb_ag[:, 1:, :]\n",
    "        # get the number of normalization transitions\n",
    "        num_transitions = mb_actions.shape[1]\n",
    "        # create the new buffer to store them\n",
    "        buffer_temp = {'obs': mb_obs, \n",
    "                       'ag': mb_ag,\n",
    "                       'g': mb_g, \n",
    "                       'actions': mb_actions, \n",
    "                       'obs_next': mb_obs_next,\n",
    "                       'ag_next': mb_ag_next,\n",
    "                       }\n",
    "        transitions = self.her_module.sample_her_transitions(buffer_temp, num_transitions)\n",
    "        obs, g = transitions['obs'], transitions['g']\n",
    "        # pre process the obs and g\n",
    "        transitions['obs'], transitions['g'] = preproc_og(obs, g)\n",
    "        # update\n",
    "        self.o_norm.update(transitions['obs'])\n",
    "        self.g_norm.update(transitions['g'])\n",
    "        # recompute the stats\n",
    "        self.o_norm.recompute_stats()\n",
    "        self.g_norm.recompute_stats()\n",
    "\n",
    "    # soft update\n",
    "    def _soft_update_target_network(self, target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_((1 - self.args.polyak) * param.data + self.args.polyak * target_param.data)\n",
    "\n",
    "    # update the network\n",
    "    def _update_network(self):\n",
    "        # sample the episodes\n",
    "        transitions = self.buffer.sample(self.args.batch_size)\n",
    "        # pre-process the observation and goal\n",
    "        o, o_next, g = transitions['obs'], transitions['obs_next'], transitions['g']\n",
    "        transitions['obs'], transitions['g'] = preproc_og(o, g)\n",
    "        transitions['obs_next'], transitions['g_next'] = preproc_og(o_next, g)\n",
    "        # start to do the update\n",
    "        obs_norm = self.o_norm.normalize(transitions['obs'])\n",
    "        g_norm = self.g_norm.normalize(transitions['g'])\n",
    "        inputs_norm = np.concatenate([obs_norm, g_norm], axis=1)\n",
    "        obs_next_norm = self.o_norm.normalize(transitions['obs_next'])\n",
    "        g_next_norm = self.g_norm.normalize(transitions['g_next'])\n",
    "        inputs_next_norm = np.concatenate([obs_next_norm, g_next_norm], axis=1)\n",
    "        # transfer them into the tensor\n",
    "        inputs_norm_tensor = torch.tensor(inputs_norm, dtype=torch.float32)\n",
    "        inputs_next_norm_tensor = torch.tensor(inputs_next_norm, dtype=torch.float32)\n",
    "        actions_tensor = torch.tensor(transitions['actions'], dtype=torch.float32)\n",
    "        r_tensor = torch.tensor(transitions['r'], dtype=torch.float32) \n",
    "        if self.args.cuda:\n",
    "            inputs_norm_tensor = inputs_norm_tensor.cuda()\n",
    "            inputs_next_norm_tensor = inputs_next_norm_tensor.cuda()\n",
    "            actions_tensor = actions_tensor.cuda()\n",
    "            r_tensor = r_tensor.cuda()\n",
    "        # calculate the target Q value function\n",
    "        with torch.no_grad():\n",
    "            # do the normalization\n",
    "            # concatenate the stuffs\n",
    "            actions_next = self.actor_target_network(inputs_next_norm_tensor)\n",
    "            q_next_value = self.critic_target_network(inputs_next_norm_tensor, actions_next)\n",
    "            q_next_value = q_next_value.detach()\n",
    "            target_q_value = r_tensor + self.args.gamma * q_next_value\n",
    "            target_q_value = target_q_value.detach()\n",
    "            # clip the q value\n",
    "            clip_return = 1 / (1 - self.args.gamma)\n",
    "            target_q_value = torch.clamp(target_q_value, -clip_return, 0)\n",
    "        # the q loss\n",
    "        real_q_value = self.critic_network(inputs_norm_tensor, actions_tensor)\n",
    "        critic_loss = (target_q_value - real_q_value).pow(2).mean()\n",
    "        # the actor loss\n",
    "        actions_real = self.actor_network(inputs_norm_tensor)\n",
    "        actor_loss = -self.critic_network(inputs_norm_tensor, actions_real).mean()\n",
    "        actor_loss += self.args.action_l2 * (actions_real / self.env_params['action_max']).pow(2).mean()\n",
    "        # start to update the network\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        sync_grads(self.actor_network)\n",
    "        self.actor_optim.step()\n",
    "        # update the critic_network\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        sync_grads(self.critic_network)\n",
    "        self.critic_optim.step()\n",
    "\n",
    "    # do the evaluation\n",
    "    def _eval_agent(self):\n",
    "        total_success_rate = []\n",
    "        for _ in range(self.args.n_test_rollouts):\n",
    "            per_success_rate = []\n",
    "            observation = self.env.reset()\n",
    "            obs = observation['observation']\n",
    "            g = observation['desired_goal']\n",
    "            for _ in range(self.env_params['max_timesteps']):\n",
    "                with torch.no_grad():\n",
    "                    input_tensor = self._preproc_inputs(obs, g)\n",
    "                    pi = self.actor_network(input_tensor)\n",
    "                    # convert the actions\n",
    "                    actions = pi.detach().cpu().numpy().squeeze()\n",
    "                observation_new, _, _, info = self.env.step(actions)\n",
    "                obs = observation_new['observation']\n",
    "                g = observation_new['desired_goal']\n",
    "                per_success_rate.append(info['is_success'])\n",
    "            total_success_rate.append(per_success_rate)\n",
    "        total_success_rate = np.array(total_success_rate)\n",
    "        local_success_rate = np.mean(total_success_rate[:, -1])\n",
    "        global_success_rate = MPI.COMM_WORLD.allreduce(local_success_rate, op=MPI.SUM)\n",
    "        return global_success_rate / MPI.COMM_WORLD.Get_size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d9613",
   "metadata": {},
   "source": [
    "### Define environmental parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d33af9ba-1943-43b6-aa40-26ac1c0e1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_params(env):\n",
    "    obs = env.reset()\n",
    "    # close the environment\n",
    "    params = {'obs': obs['observation'].shape[0],\n",
    "            'goal': obs['desired_goal'].shape[0],\n",
    "            'action': env.action_space.shape[0],\n",
    "            'action_max': env.action_space.high[0],\n",
    "            }\n",
    "    params['max_timesteps'] = env._max_episode_steps\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44770cf",
   "metadata": {},
   "source": [
    "### Parse arguments and run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f2d10c28-4e5a-48ec-aab2-e0b04352f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch(args, path=None):\n",
    "    # Save path\n",
    "    save_path = os.path.join(path,f'{args.env_name}.wts')\n",
    "    # create the ddpg_agent\n",
    "    env = gym.make(args.env_name)\n",
    "    # get the environment parameters\n",
    "    env_params = get_env_params(env)\n",
    "    # create the ddpg agent to interact with the environment \n",
    "    ddpg_trainer = ddpg_agent(args, env, env_params)\n",
    "    agent_weights, onorm, gnorm = ddpg_trainer.learn()\n",
    "    # Save weights here\n",
    "    # torch.save(agent_weights.state_dict(), save_path)\n",
    "    # Save extra essential pieces\n",
    "    torch.save([onorm.mean, onorm.std, gnorm.mean, gnorm.std, agent_weights.state_dict()], save_path)\n",
    "    return agent_weights, onorm, gnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a785f",
   "metadata": {},
   "source": [
    "### TODO: Cleanup the way code is launched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7f3a8055-9301-4e74-b6ab-49e3703e0965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8839ae2d394cbebdebae0d61fb6618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-12-05 01:07:55.256406] epoch is: 0, eval success rate is: 0.000\n",
      "[2021-12-05 01:08:11.380106] epoch is: 1, eval success rate is: 0.000\n",
      "[2021-12-05 01:08:27.397366] epoch is: 2, eval success rate is: 0.000\n",
      "[2021-12-05 01:08:43.340784] epoch is: 3, eval success rate is: 0.000\n",
      "[2021-12-05 01:08:59.445148] epoch is: 4, eval success rate is: 0.000\n",
      "[2021-12-05 01:09:15.736169] epoch is: 5, eval success rate is: 0.000\n",
      "[2021-12-05 01:09:32.457534] epoch is: 6, eval success rate is: 0.000\n",
      "[2021-12-05 01:09:49.274115] epoch is: 7, eval success rate is: 0.100\n",
      "[2021-12-05 01:10:05.956112] epoch is: 8, eval success rate is: 0.000\n",
      "[2021-12-05 01:10:22.225958] epoch is: 9, eval success rate is: 0.000\n",
      "[2021-12-05 01:10:38.571040] epoch is: 10, eval success rate is: 0.000\n",
      "[2021-12-05 01:10:54.817923] epoch is: 11, eval success rate is: 0.000\n",
      "[2021-12-05 01:11:11.262048] epoch is: 12, eval success rate is: 0.000\n",
      "[2021-12-05 01:11:27.692382] epoch is: 13, eval success rate is: 0.100\n",
      "[2021-12-05 01:11:43.912770] epoch is: 14, eval success rate is: 0.000\n",
      "[2021-12-05 01:12:00.109501] epoch is: 15, eval success rate is: 0.000\n",
      "[2021-12-05 01:12:16.400731] epoch is: 16, eval success rate is: 0.200\n",
      "[2021-12-05 01:12:32.704481] epoch is: 17, eval success rate is: 0.100\n",
      "[2021-12-05 01:12:48.732155] epoch is: 18, eval success rate is: 0.200\n",
      "[2021-12-05 01:13:04.827970] epoch is: 19, eval success rate is: 0.000\n",
      "[2021-12-05 01:13:21.022855] epoch is: 20, eval success rate is: 0.000\n",
      "[2021-12-05 01:13:37.046034] epoch is: 21, eval success rate is: 0.100\n",
      "[2021-12-05 01:13:53.246145] epoch is: 22, eval success rate is: 0.000\n",
      "[2021-12-05 01:14:10.178798] epoch is: 23, eval success rate is: 0.000\n",
      "[2021-12-05 01:14:27.047549] epoch is: 24, eval success rate is: 0.000\n",
      "[2021-12-05 01:14:43.624764] epoch is: 25, eval success rate is: 0.100\n",
      "[2021-12-05 01:15:00.342966] epoch is: 26, eval success rate is: 0.100\n",
      "[2021-12-05 01:15:17.068188] epoch is: 27, eval success rate is: 0.000\n",
      "[2021-12-05 01:15:35.346540] epoch is: 28, eval success rate is: 0.000\n",
      "[2021-12-05 01:15:54.065853] epoch is: 29, eval success rate is: 0.000\n",
      "[2021-12-05 01:16:13.380727] epoch is: 30, eval success rate is: 0.100\n",
      "[2021-12-05 01:16:32.968301] epoch is: 31, eval success rate is: 0.000\n",
      "[2021-12-05 01:16:52.188412] epoch is: 32, eval success rate is: 0.100\n",
      "[2021-12-05 01:17:10.734857] epoch is: 33, eval success rate is: 0.100\n",
      "[2021-12-05 01:17:29.250330] epoch is: 34, eval success rate is: 0.100\n",
      "[2021-12-05 01:17:47.690469] epoch is: 35, eval success rate is: 0.300\n",
      "[2021-12-05 01:18:06.452007] epoch is: 36, eval success rate is: 0.000\n",
      "[2021-12-05 01:18:25.060483] epoch is: 37, eval success rate is: 0.000\n",
      "[2021-12-05 01:18:43.516342] epoch is: 38, eval success rate is: 0.200\n",
      "[2021-12-05 01:19:02.108359] epoch is: 39, eval success rate is: 0.100\n",
      "[2021-12-05 01:19:20.686711] epoch is: 40, eval success rate is: 0.300\n",
      "[2021-12-05 01:19:39.183287] epoch is: 41, eval success rate is: 0.300\n",
      "[2021-12-05 01:19:57.815256] epoch is: 42, eval success rate is: 0.000\n",
      "[2021-12-05 01:20:16.476425] epoch is: 43, eval success rate is: 0.100\n",
      "[2021-12-05 01:20:35.047154] epoch is: 44, eval success rate is: 0.100\n",
      "[2021-12-05 01:20:53.565858] epoch is: 45, eval success rate is: 0.000\n",
      "[2021-12-05 01:21:12.099071] epoch is: 46, eval success rate is: 0.100\n",
      "[2021-12-05 01:21:31.023422] epoch is: 47, eval success rate is: 0.300\n",
      "[2021-12-05 01:21:49.822884] epoch is: 48, eval success rate is: 0.100\n",
      "[2021-12-05 01:22:08.449051] epoch is: 49, eval success rate is: 0.200\n",
      "[2021-12-05 01:22:27.314618] epoch is: 50, eval success rate is: 0.500\n",
      "[2021-12-05 01:22:45.767733] epoch is: 51, eval success rate is: 0.300\n",
      "[2021-12-05 01:23:04.200575] epoch is: 52, eval success rate is: 0.400\n",
      "[2021-12-05 01:23:22.757055] epoch is: 53, eval success rate is: 0.400\n",
      "[2021-12-05 01:23:41.376621] epoch is: 54, eval success rate is: 0.200\n",
      "[2021-12-05 01:24:00.034422] epoch is: 55, eval success rate is: 0.500\n",
      "[2021-12-05 01:24:18.476022] epoch is: 56, eval success rate is: 0.100\n",
      "[2021-12-05 01:24:36.882154] epoch is: 57, eval success rate is: 0.300\n",
      "[2021-12-05 01:24:55.274558] epoch is: 58, eval success rate is: 0.200\n",
      "[2021-12-05 01:25:13.836108] epoch is: 59, eval success rate is: 0.600\n",
      "[2021-12-05 01:25:32.216125] epoch is: 60, eval success rate is: 0.200\n",
      "[2021-12-05 01:25:50.830659] epoch is: 61, eval success rate is: 0.400\n",
      "[2021-12-05 01:26:09.627641] epoch is: 62, eval success rate is: 0.500\n",
      "[2021-12-05 01:26:28.143219] epoch is: 63, eval success rate is: 0.400\n",
      "[2021-12-05 01:26:46.621871] epoch is: 64, eval success rate is: 0.200\n",
      "[2021-12-05 01:27:05.118002] epoch is: 65, eval success rate is: 0.600\n",
      "[2021-12-05 01:27:23.510880] epoch is: 66, eval success rate is: 0.300\n",
      "[2021-12-05 01:27:42.400860] epoch is: 67, eval success rate is: 0.600\n",
      "[2021-12-05 01:28:00.806541] epoch is: 68, eval success rate is: 0.400\n",
      "[2021-12-05 01:28:19.289593] epoch is: 69, eval success rate is: 0.400\n",
      "[2021-12-05 01:28:37.906525] epoch is: 70, eval success rate is: 0.200\n",
      "[2021-12-05 01:28:56.402674] epoch is: 71, eval success rate is: 0.300\n",
      "[2021-12-05 01:29:14.974943] epoch is: 72, eval success rate is: 0.600\n",
      "[2021-12-05 01:29:33.546676] epoch is: 73, eval success rate is: 0.400\n",
      "[2021-12-05 01:29:52.172109] epoch is: 74, eval success rate is: 0.200\n",
      "[2021-12-05 01:30:10.691778] epoch is: 75, eval success rate is: 0.300\n",
      "[2021-12-05 01:30:29.222780] epoch is: 76, eval success rate is: 0.500\n",
      "[2021-12-05 01:30:47.882887] epoch is: 77, eval success rate is: 0.300\n",
      "[2021-12-05 01:31:06.310365] epoch is: 78, eval success rate is: 0.800\n",
      "[2021-12-05 01:31:24.685098] epoch is: 79, eval success rate is: 0.400\n",
      "[2021-12-05 01:31:43.094787] epoch is: 80, eval success rate is: 0.500\n",
      "[2021-12-05 01:32:01.452891] epoch is: 81, eval success rate is: 0.000\n",
      "[2021-12-05 01:32:19.869391] epoch is: 82, eval success rate is: 0.300\n",
      "[2021-12-05 01:32:38.301014] epoch is: 83, eval success rate is: 0.500\n",
      "[2021-12-05 01:32:56.756200] epoch is: 84, eval success rate is: 0.300\n",
      "[2021-12-05 01:33:15.281169] epoch is: 85, eval success rate is: 0.200\n",
      "[2021-12-05 01:33:33.680986] epoch is: 86, eval success rate is: 0.500\n",
      "[2021-12-05 01:33:52.252182] epoch is: 87, eval success rate is: 0.300\n",
      "[2021-12-05 01:34:11.257470] epoch is: 88, eval success rate is: 0.300\n",
      "[2021-12-05 01:34:30.124155] epoch is: 89, eval success rate is: 0.300\n",
      "[2021-12-05 01:34:49.119998] epoch is: 90, eval success rate is: 0.300\n",
      "[2021-12-05 01:35:07.867663] epoch is: 91, eval success rate is: 0.000\n",
      "[2021-12-05 01:35:26.473758] epoch is: 92, eval success rate is: 0.300\n",
      "[2021-12-05 01:35:44.940179] epoch is: 93, eval success rate is: 0.600\n",
      "[2021-12-05 01:36:03.454192] epoch is: 94, eval success rate is: 0.400\n",
      "[2021-12-05 01:36:22.155330] epoch is: 95, eval success rate is: 0.400\n",
      "[2021-12-05 01:36:40.795704] epoch is: 96, eval success rate is: 0.200\n",
      "[2021-12-05 01:36:59.491328] epoch is: 97, eval success rate is: 0.700\n",
      "[2021-12-05 01:37:18.241065] epoch is: 98, eval success rate is: 0.400\n",
      "[2021-12-05 01:37:36.932401] epoch is: 99, eval success rate is: 0.500\n",
      "[2021-12-05 01:37:55.565489] epoch is: 100, eval success rate is: 0.800\n",
      "[2021-12-05 01:38:14.270960] epoch is: 101, eval success rate is: 0.400\n",
      "[2021-12-05 01:38:32.793041] epoch is: 102, eval success rate is: 0.300\n",
      "[2021-12-05 01:38:51.424518] epoch is: 103, eval success rate is: 0.100\n",
      "[2021-12-05 01:39:10.155417] epoch is: 104, eval success rate is: 0.400\n",
      "[2021-12-05 01:39:28.778845] epoch is: 105, eval success rate is: 0.400\n",
      "[2021-12-05 01:39:47.758692] epoch is: 106, eval success rate is: 0.200\n",
      "[2021-12-05 01:40:06.595720] epoch is: 107, eval success rate is: 0.300\n",
      "[2021-12-05 01:40:25.145655] epoch is: 108, eval success rate is: 0.300\n",
      "[2021-12-05 01:40:43.592748] epoch is: 109, eval success rate is: 0.200\n",
      "[2021-12-05 01:41:02.216299] epoch is: 110, eval success rate is: 0.300\n",
      "[2021-12-05 01:41:20.704615] epoch is: 111, eval success rate is: 0.400\n",
      "[2021-12-05 01:41:39.117994] epoch is: 112, eval success rate is: 0.800\n",
      "[2021-12-05 01:41:57.604805] epoch is: 113, eval success rate is: 0.300\n",
      "[2021-12-05 01:42:16.196515] epoch is: 114, eval success rate is: 0.000\n",
      "[2021-12-05 01:42:34.651631] epoch is: 115, eval success rate is: 0.500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-12-05 01:42:53.122437] epoch is: 116, eval success rate is: 0.200\n",
      "[2021-12-05 01:43:11.663747] epoch is: 117, eval success rate is: 0.200\n",
      "[2021-12-05 01:43:30.535461] epoch is: 118, eval success rate is: 0.400\n",
      "[2021-12-05 01:43:49.245096] epoch is: 119, eval success rate is: 0.600\n",
      "[2021-12-05 01:44:08.061266] epoch is: 120, eval success rate is: 0.500\n",
      "[2021-12-05 01:44:26.751772] epoch is: 121, eval success rate is: 0.300\n",
      "[2021-12-05 01:44:45.348890] epoch is: 122, eval success rate is: 0.400\n",
      "[2021-12-05 01:45:04.139325] epoch is: 123, eval success rate is: 0.300\n",
      "[2021-12-05 01:45:22.780334] epoch is: 124, eval success rate is: 0.300\n",
      "[2021-12-05 01:45:41.335508] epoch is: 125, eval success rate is: 0.300\n",
      "[2021-12-05 01:46:00.041687] epoch is: 126, eval success rate is: 0.400\n",
      "[2021-12-05 01:46:18.630781] epoch is: 127, eval success rate is: 0.200\n",
      "[2021-12-05 01:46:37.218577] epoch is: 128, eval success rate is: 0.400\n",
      "[2021-12-05 01:46:55.842736] epoch is: 129, eval success rate is: 0.400\n",
      "[2021-12-05 01:47:14.532581] epoch is: 130, eval success rate is: 0.800\n",
      "[2021-12-05 01:47:33.214757] epoch is: 131, eval success rate is: 0.000\n",
      "[2021-12-05 01:47:51.874292] epoch is: 132, eval success rate is: 0.300\n",
      "[2021-12-05 01:48:10.618686] epoch is: 133, eval success rate is: 0.200\n",
      "[2021-12-05 01:48:29.150261] epoch is: 134, eval success rate is: 0.300\n",
      "[2021-12-05 01:48:47.761589] epoch is: 135, eval success rate is: 0.300\n",
      "[2021-12-05 01:49:06.753827] epoch is: 136, eval success rate is: 0.700\n",
      "[2021-12-05 01:49:25.266859] epoch is: 137, eval success rate is: 0.400\n",
      "[2021-12-05 01:49:43.814788] epoch is: 138, eval success rate is: 0.500\n",
      "[2021-12-05 01:50:02.425747] epoch is: 139, eval success rate is: 0.400\n",
      "[2021-12-05 01:50:21.739312] epoch is: 140, eval success rate is: 0.200\n",
      "[2021-12-05 01:50:40.403729] epoch is: 141, eval success rate is: 0.400\n",
      "[2021-12-05 01:50:59.137961] epoch is: 142, eval success rate is: 1.000\n",
      "[2021-12-05 01:51:17.924196] epoch is: 143, eval success rate is: 0.400\n",
      "[2021-12-05 01:51:36.701156] epoch is: 144, eval success rate is: 0.400\n",
      "[2021-12-05 01:51:55.421470] epoch is: 145, eval success rate is: 0.700\n",
      "[2021-12-05 01:52:14.196479] epoch is: 146, eval success rate is: 0.300\n",
      "[2021-12-05 01:52:32.941951] epoch is: 147, eval success rate is: 0.600\n",
      "[2021-12-05 01:52:51.601485] epoch is: 148, eval success rate is: 0.700\n",
      "[2021-12-05 01:53:10.334998] epoch is: 149, eval success rate is: 0.500\n",
      "[2021-12-05 01:53:29.006186] epoch is: 150, eval success rate is: 0.600\n",
      "[2021-12-05 01:53:47.548685] epoch is: 151, eval success rate is: 0.500\n",
      "[2021-12-05 01:54:06.257959] epoch is: 152, eval success rate is: 0.300\n",
      "[2021-12-05 01:54:24.900820] epoch is: 153, eval success rate is: 0.200\n",
      "[2021-12-05 01:54:43.604752] epoch is: 154, eval success rate is: 0.700\n",
      "[2021-12-05 01:55:02.141152] epoch is: 155, eval success rate is: 0.200\n",
      "[2021-12-05 01:55:20.872893] epoch is: 156, eval success rate is: 0.700\n",
      "[2021-12-05 01:55:39.554769] epoch is: 157, eval success rate is: 0.400\n",
      "[2021-12-05 01:55:58.038884] epoch is: 158, eval success rate is: 0.600\n",
      "[2021-12-05 01:56:17.057828] epoch is: 159, eval success rate is: 0.400\n",
      "[2021-12-05 01:56:36.112990] epoch is: 160, eval success rate is: 0.600\n",
      "[2021-12-05 01:56:54.852709] epoch is: 161, eval success rate is: 0.500\n",
      "[2021-12-05 01:57:13.650559] epoch is: 162, eval success rate is: 0.400\n",
      "[2021-12-05 01:57:32.213817] epoch is: 163, eval success rate is: 0.400\n",
      "[2021-12-05 01:57:50.637170] epoch is: 164, eval success rate is: 0.600\n",
      "[2021-12-05 01:58:09.125609] epoch is: 165, eval success rate is: 0.500\n",
      "[2021-12-05 01:58:27.838698] epoch is: 166, eval success rate is: 0.200\n",
      "[2021-12-05 01:58:46.569112] epoch is: 167, eval success rate is: 0.500\n",
      "[2021-12-05 01:59:05.304126] epoch is: 168, eval success rate is: 0.400\n",
      "[2021-12-05 01:59:23.744756] epoch is: 169, eval success rate is: 0.600\n",
      "[2021-12-05 01:59:42.203593] epoch is: 170, eval success rate is: 0.600\n",
      "[2021-12-05 02:00:00.891029] epoch is: 171, eval success rate is: 0.500\n",
      "[2021-12-05 02:00:19.371876] epoch is: 172, eval success rate is: 0.500\n",
      "[2021-12-05 02:00:38.059973] epoch is: 173, eval success rate is: 0.200\n",
      "[2021-12-05 02:00:56.565405] epoch is: 174, eval success rate is: 0.600\n",
      "[2021-12-05 02:01:15.754229] epoch is: 175, eval success rate is: 0.400\n",
      "[2021-12-05 02:01:34.526191] epoch is: 176, eval success rate is: 0.600\n",
      "[2021-12-05 02:01:53.028310] epoch is: 177, eval success rate is: 0.300\n",
      "[2021-12-05 02:02:11.779214] epoch is: 178, eval success rate is: 0.200\n",
      "[2021-12-05 02:02:30.448333] epoch is: 179, eval success rate is: 0.500\n",
      "[2021-12-05 02:02:49.038682] epoch is: 180, eval success rate is: 0.300\n",
      "[2021-12-05 02:03:07.505259] epoch is: 181, eval success rate is: 0.500\n",
      "[2021-12-05 02:03:26.222706] epoch is: 182, eval success rate is: 0.300\n",
      "[2021-12-05 02:03:44.943882] epoch is: 183, eval success rate is: 0.600\n",
      "[2021-12-05 02:04:03.539458] epoch is: 184, eval success rate is: 0.600\n",
      "[2021-12-05 02:04:21.955392] epoch is: 185, eval success rate is: 0.600\n",
      "[2021-12-05 02:04:40.434387] epoch is: 186, eval success rate is: 0.600\n",
      "[2021-12-05 02:04:58.937660] epoch is: 187, eval success rate is: 0.700\n",
      "[2021-12-05 02:05:17.364296] epoch is: 188, eval success rate is: 0.200\n",
      "[2021-12-05 02:05:35.887642] epoch is: 189, eval success rate is: 0.400\n",
      "[2021-12-05 02:05:54.534448] epoch is: 190, eval success rate is: 0.400\n",
      "[2021-12-05 02:06:13.275118] epoch is: 191, eval success rate is: 0.600\n",
      "[2021-12-05 02:06:32.009724] epoch is: 192, eval success rate is: 0.500\n",
      "[2021-12-05 02:06:50.579508] epoch is: 193, eval success rate is: 0.300\n",
      "[2021-12-05 02:07:09.354647] epoch is: 194, eval success rate is: 0.400\n",
      "[2021-12-05 02:07:28.177986] epoch is: 195, eval success rate is: 0.400\n",
      "[2021-12-05 02:07:46.739320] epoch is: 196, eval success rate is: 0.600\n",
      "[2021-12-05 02:08:05.514754] epoch is: 197, eval success rate is: 0.300\n",
      "[2021-12-05 02:08:24.217113] epoch is: 198, eval success rate is: 0.000\n",
      "[2021-12-05 02:08:42.783519] epoch is: 199, eval success rate is: 0.700\n"
     ]
    }
   ],
   "source": [
    "args_to_parse = '--env-name FetchSlide-v1 --n-epochs=200'\n",
    "args = get_args(args_to_parse)\n",
    "out = launch(args, fullPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fadb2952-c329-4ad5-b882-9784a43fa5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click a button to run a random policy:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3ce246c7d745f48da9e49a5a21e4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='push', style=ButtonStyle()), Button(description='reach', style=ButtonStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not create policy - running random policy\n",
      "[Errno 2] No such file or directory: 'FetchSlide-v1.pts'\n"
     ]
    }
   ],
   "source": [
    "#  Jupyter UI - Trigger policies for individual agents\n",
    "def button_callback(button):\n",
    "    for b in buttons:\n",
    "        b.disabled = True\n",
    "\n",
    "    env = envs[button.description]\n",
    "    final_policy = None\n",
    "    try:\n",
    "        env_name = envs[button.description]['model'].env.spec.id\n",
    "        model = torch.load(f'{env_name}.pts')\n",
    "        model_weights = model[4]\n",
    "        # Create policy using function\n",
    "        def policy(state):\n",
    "            # reset the environment\n",
    "            obs = state['observation']\n",
    "            g = state['desired_goal']\n",
    "            o_norm = normalizer(size=10, default_clip_range=200)\n",
    "            o_norm.mean = model[0]\n",
    "            o_norm.std = model[1]\n",
    "            g_norm = normalizer(size=3, default_clip_range=200)\n",
    "            g_norm.mean = model[2]\n",
    "            g_norm.std = model[3]\n",
    "            obs_norm = o_norm.normalize(obs)\n",
    "            g_norm = g_norm.normalize(g)\n",
    "            # concatenate the stuffs\n",
    "            input_tensor = np.concatenate([obs_norm, g_norm])\n",
    "            input_tensor = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "            action = pi.cpu().numpy().squeeze()\n",
    "            # add the gaussian\n",
    "            action = np.clip(action, -1, 1)\n",
    "            return action\n",
    "        final_policy = policy\n",
    "    except Exception as e:\n",
    "        print('Could not create policy - running random policy')\n",
    "        print(str(e))\n",
    "    \n",
    "    render(env, final_policy)\n",
    "    env.close()\n",
    "        \n",
    "    for b in buttons:\n",
    "        b.disabled = False\n",
    "\n",
    "buttons = []\n",
    "for env_id in envs.keys():\n",
    "    button = widgets.Button(description=env_id)\n",
    "    button.on_click(button_callback)\n",
    "    buttons.append(button)\n",
    "\n",
    "print('Click a button to run a random policy:')\n",
    "b = widgets.HBox(buttons)\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a209a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
