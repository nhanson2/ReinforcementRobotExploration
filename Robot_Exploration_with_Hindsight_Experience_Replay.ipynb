{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da504c5",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Final Project: Exploration Strategies for Robotic Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5306d8",
   "metadata": {},
   "source": [
    "### Import helper libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9b0cc99-0fb3-41eb-8385-43664701355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important trick to keep JupyterLab from crashing\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import numpy as np\n",
    "import gym\n",
    "import sys\n",
    "import traceback\n",
    "from arguments import get_args\n",
    "import random\n",
    "import torch\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "from mpi4py import MPI\n",
    "from mpi_utils.mpi_utils import sync_networks, sync_grads\n",
    "from rl_modules.replay_buffer import replay_buffer\n",
    "from rl_modules.models import actor, critic\n",
    "from mpi_utils.normalizer import normalizer\n",
    "from her_modules.her import her_sampler\n",
    "import copy\n",
    "import math\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import more_itertools as mitt\n",
    "import pygame\n",
    "import glfw\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = [36, 4]\n",
    "fullPath = str(Path('.').absolute())\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['IN_MPI'] = '1'\n",
    "\n",
    "### Create environments and place to store model weights\n",
    "envs = {\n",
    "    'push': {\n",
    "        'model': gym.make('FetchPush-v1'),\n",
    "        'weights': None\n",
    "    },\n",
    "    'reach': {\n",
    "        'model': gym.make('FetchReach-v1'),\n",
    "        'weights': None\n",
    "    },\n",
    "    'slide': {\n",
    "        'model': gym.make('FetchSlide-v1'),\n",
    "        'weights': None\n",
    "    },\n",
    "    'pick': {\n",
    "        'model': gym.make('FetchPickAndPlace-v1'),\n",
    "        'weights': None\n",
    "    }\n",
    "}\n",
    "\n",
    "### List of Exploration Policies\n",
    "exploration_policies = {\n",
    "    'e-greedy': None,\n",
    "    'semi-uniform-distributed': None,\n",
    "    'boltzmann-distributed': None,\n",
    "    'UCB': None,\n",
    "    'counter-based': None,\n",
    "    'counter-based-decay': None,\n",
    "    'recency-based': None,\n",
    "    'OFU': None,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3954211",
   "metadata": {},
   "source": [
    "### Graphically Render Policy\n",
    "\n",
    "Using Mujoco backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c8f5f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env, policy=None):\n",
    "    \"\"\"Graphically render an episode using the given policy\n",
    "\n",
    "    :param env:  Gym environment\n",
    "    :param policy:  function which maps state to action.  If None, the random\n",
    "                    policy is used.\n",
    "    \"\"\"\n",
    "    glfw.init()\n",
    "    if policy is None:\n",
    "\n",
    "        def policy(state):\n",
    "            return env.action_space.sample()\n",
    "\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    i = 0\n",
    "    while i < 3000:\n",
    "        action = policy(state)\n",
    "        state, _, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "        i += 1\n",
    "            \n",
    "    env.close()\n",
    "    glfw.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736dc67",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de59111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_og(o, g):\n",
    "    o = np.clip(o, -200, 200)\n",
    "    g = np.clip(g, -200, 200)\n",
    "    return o, g\n",
    "\n",
    "# pre_process the inputs\n",
    "def preproc_inputs(obs, g):\n",
    "    obs_norm = self.o_norm.normalize(obs)\n",
    "    g_norm = self.g_norm.normalize(g)\n",
    "    # concatenate the stuffs\n",
    "    inputs = np.concatenate([obs_norm, g_norm])\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b8072a",
   "metadata": {},
   "source": [
    "### Define Structure for Deep Deterministic Policy Gradient (DDPG) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5496d8a-67e0-45af-9871-e859ecbca084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ddpg with HER (MPI-version)\n",
    "\n",
    "\"\"\"\n",
    "class ddpg_agent:\n",
    "    def __init__(self, args, env, env_params):\n",
    "        self.args = args\n",
    "        self.env = env\n",
    "        self.env_params = env_params\n",
    "        # create the network\n",
    "        self.actor_network = actor(env_params)\n",
    "        self.critic_network = critic(env_params)\n",
    "        # sync the networks across the cpus\n",
    "        sync_networks(self.actor_network)\n",
    "        sync_networks(self.critic_network)\n",
    "        # build up the target network\n",
    "        self.actor_target_network = actor(env_params)\n",
    "        self.critic_target_network = critic(env_params)\n",
    "        # load the weights into the target networks\n",
    "        self.actor_target_network.load_state_dict(self.actor_network.state_dict())\n",
    "        self.critic_target_network.load_state_dict(self.critic_network.state_dict())\n",
    "        # if use gpu\n",
    "        if self.args.cuda:\n",
    "            self.actor_network.cuda()\n",
    "            self.critic_network.cuda()\n",
    "            self.actor_target_network.cuda()\n",
    "            self.critic_target_network.cuda()\n",
    "        # create the optimizer\n",
    "        self.actor_optim = torch.optim.Adam(self.actor_network.parameters(), lr=self.args.lr_actor)\n",
    "        self.critic_optim = torch.optim.Adam(self.critic_network.parameters(), lr=self.args.lr_critic)\n",
    "        # her sampler\n",
    "        self.her_module = her_sampler(self.args.replay_strategy, self.args.replay_k, self.env.compute_reward)\n",
    "        # create the replay buffer\n",
    "        self.buffer = replay_buffer(self.env_params, self.args.buffer_size, self.her_module.sample_her_transitions)\n",
    "        # create the normalizer\n",
    "        self.o_norm = normalizer(size=env_params['obs'], default_clip_range=self.args.clip_range)\n",
    "        self.g_norm = normalizer(size=env_params['goal'], default_clip_range=self.args.clip_range)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        train the network\n",
    "\n",
    "        \"\"\"\n",
    "        # start to collect samples\n",
    "        pbar = tqdm.notebook.trange(self.args.n_epochs)\n",
    "        for epoch in pbar:\n",
    "            for _ in range(self.args.n_cycles):\n",
    "                mb_obs, mb_ag, mb_g, mb_actions = [], [], [], []\n",
    "                for _ in range(self.args.num_rollouts_per_mpi):\n",
    "                    # reset the rollouts\n",
    "                    ep_obs, ep_ag, ep_g, ep_actions = [], [], [], []\n",
    "                    # reset the environment\n",
    "                    observation = self.env.reset()\n",
    "                    obs = observation['observation']\n",
    "                    ag = observation['achieved_goal']\n",
    "                    g = observation['desired_goal']\n",
    "                    # start to collect samples\n",
    "                    for t in range(self.env_params['max_timesteps']):\n",
    "                        with torch.no_grad():\n",
    "                            input_tensor = self._preproc_inputs(obs, g)\n",
    "                            pi = self.actor_network(input_tensor)\n",
    "                            action = self._select_actions(pi)\n",
    "                        # feed the actions into the environment\n",
    "                        observation_new, _, _, info = self.env.step(action)\n",
    "                        obs_new = observation_new['observation']\n",
    "                        ag_new = observation_new['achieved_goal']\n",
    "                        # append rollouts\n",
    "                        ep_obs.append(obs.copy())\n",
    "                        ep_ag.append(ag.copy())\n",
    "                        ep_g.append(g.copy())\n",
    "                        ep_actions.append(action.copy())\n",
    "                        # re-assign the observation\n",
    "                        obs = obs_new\n",
    "                        ag = ag_new\n",
    "                    ep_obs.append(obs.copy())\n",
    "                    ep_ag.append(ag.copy())\n",
    "                    mb_obs.append(ep_obs)\n",
    "                    mb_ag.append(ep_ag)\n",
    "                    mb_g.append(ep_g)\n",
    "                    mb_actions.append(ep_actions)\n",
    "                # convert them into arrays\n",
    "                mb_obs = np.array(mb_obs)\n",
    "                mb_ag = np.array(mb_ag)\n",
    "                mb_g = np.array(mb_g)\n",
    "                mb_actions = np.array(mb_actions)\n",
    "                # store the episodes\n",
    "                self.buffer.store_episode([mb_obs, mb_ag, mb_g, mb_actions])\n",
    "                self._update_normalizer([mb_obs, mb_ag, mb_g, mb_actions])\n",
    "                for _ in range(self.args.n_batches):\n",
    "                    # train the network\n",
    "                    self._update_network()\n",
    "                # soft update\n",
    "                self._soft_update_target_network(self.actor_target_network, self.actor_network)\n",
    "                self._soft_update_target_network(self.critic_target_network, self.critic_network)\n",
    "            # start to do the evaluation\n",
    "            success_rate = self._eval_agent()\n",
    "            if MPI.COMM_WORLD.Get_rank() == 0:\n",
    "                print('[{}] epoch is: {}, eval success rate is: {:.3f}'.format(datetime.now(), epoch, success_rate))\n",
    "\n",
    "        # Return the network weights\n",
    "        #torch.save([self.o_norm.mean, self.o_norm.std, self.g_norm.mean, self.g_norm.std, self.actor_network.state_dict()], \\\n",
    "        #    self.model_path + '/model.pt')\n",
    "        return self.actor_network, self.o_norm, self.g_norm\n",
    "\n",
    "    # pre_process the inputs\n",
    "    def _preproc_inputs(self, obs, g):\n",
    "        obs_norm = self.o_norm.normalize(obs)\n",
    "        g_norm = self.g_norm.normalize(g)\n",
    "        # concatenate the stuffs\n",
    "        inputs = np.concatenate([obs_norm, g_norm])\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "        if self.args.cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        return inputs\n",
    "    \n",
    "    # this function will choose action for the agent and do the exploration\n",
    "    def _select_actions(self, pi):\n",
    "        action = pi.cpu().numpy().squeeze()\n",
    "        # add the gaussian\n",
    "        action += self.args.noise_eps * self.env_params['action_max'] * np.random.randn(*action.shape)\n",
    "        action = np.clip(action, -self.env_params['action_max'], self.env_params['action_max'])\n",
    "        # random actions...\n",
    "        random_actions = np.random.uniform(low=-self.env_params['action_max'], high=self.env_params['action_max'], \\\n",
    "                                            size=self.env_params['action'])\n",
    "        # choose if use the random actions\n",
    "        action += np.random.binomial(1, self.args.random_eps, 1)[0] * (random_actions - action)\n",
    "        return action\n",
    "\n",
    "    # update the normalizer\n",
    "    def _update_normalizer(self, episode_batch):\n",
    "        mb_obs, mb_ag, mb_g, mb_actions = episode_batch\n",
    "        mb_obs_next = mb_obs[:, 1:, :]\n",
    "        mb_ag_next = mb_ag[:, 1:, :]\n",
    "        # get the number of normalization transitions\n",
    "        num_transitions = mb_actions.shape[1]\n",
    "        # create the new buffer to store them\n",
    "        buffer_temp = {'obs': mb_obs, \n",
    "                       'ag': mb_ag,\n",
    "                       'g': mb_g, \n",
    "                       'actions': mb_actions, \n",
    "                       'obs_next': mb_obs_next,\n",
    "                       'ag_next': mb_ag_next,\n",
    "                       }\n",
    "        transitions = self.her_module.sample_her_transitions(buffer_temp, num_transitions)\n",
    "        obs, g = transitions['obs'], transitions['g']\n",
    "        # pre process the obs and g\n",
    "        transitions['obs'], transitions['g'] = preproc_og(obs, g)\n",
    "        # update\n",
    "        self.o_norm.update(transitions['obs'])\n",
    "        self.g_norm.update(transitions['g'])\n",
    "        # recompute the stats\n",
    "        self.o_norm.recompute_stats()\n",
    "        self.g_norm.recompute_stats()\n",
    "\n",
    "    # soft update\n",
    "    def _soft_update_target_network(self, target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_((1 - self.args.polyak) * param.data + self.args.polyak * target_param.data)\n",
    "\n",
    "    # update the network\n",
    "    def _update_network(self):\n",
    "        # sample the episodes\n",
    "        transitions = self.buffer.sample(self.args.batch_size)\n",
    "        # pre-process the observation and goal\n",
    "        o, o_next, g = transitions['obs'], transitions['obs_next'], transitions['g']\n",
    "        transitions['obs'], transitions['g'] = preproc_og(o, g)\n",
    "        transitions['obs_next'], transitions['g_next'] = preproc_og(o_next, g)\n",
    "        # start to do the update\n",
    "        obs_norm = self.o_norm.normalize(transitions['obs'])\n",
    "        g_norm = self.g_norm.normalize(transitions['g'])\n",
    "        inputs_norm = np.concatenate([obs_norm, g_norm], axis=1)\n",
    "        obs_next_norm = self.o_norm.normalize(transitions['obs_next'])\n",
    "        g_next_norm = self.g_norm.normalize(transitions['g_next'])\n",
    "        inputs_next_norm = np.concatenate([obs_next_norm, g_next_norm], axis=1)\n",
    "        # transfer them into the tensor\n",
    "        inputs_norm_tensor = torch.tensor(inputs_norm, dtype=torch.float32)\n",
    "        inputs_next_norm_tensor = torch.tensor(inputs_next_norm, dtype=torch.float32)\n",
    "        actions_tensor = torch.tensor(transitions['actions'], dtype=torch.float32)\n",
    "        r_tensor = torch.tensor(transitions['r'], dtype=torch.float32) \n",
    "        if self.args.cuda:\n",
    "            inputs_norm_tensor = inputs_norm_tensor.cuda()\n",
    "            inputs_next_norm_tensor = inputs_next_norm_tensor.cuda()\n",
    "            actions_tensor = actions_tensor.cuda()\n",
    "            r_tensor = r_tensor.cuda()\n",
    "        # calculate the target Q value function\n",
    "        with torch.no_grad():\n",
    "            # do the normalization\n",
    "            # concatenate the stuffs\n",
    "            actions_next = self.actor_target_network(inputs_next_norm_tensor)\n",
    "            q_next_value = self.critic_target_network(inputs_next_norm_tensor, actions_next)\n",
    "            q_next_value = q_next_value.detach()\n",
    "            target_q_value = r_tensor + self.args.gamma * q_next_value\n",
    "            target_q_value = target_q_value.detach()\n",
    "            # clip the q value\n",
    "            clip_return = 1 / (1 - self.args.gamma)\n",
    "            target_q_value = torch.clamp(target_q_value, -clip_return, 0)\n",
    "        # the q loss\n",
    "        real_q_value = self.critic_network(inputs_norm_tensor, actions_tensor)\n",
    "        critic_loss = (target_q_value - real_q_value).pow(2).mean()\n",
    "        # the actor loss\n",
    "        actions_real = self.actor_network(inputs_norm_tensor)\n",
    "        actor_loss = -self.critic_network(inputs_norm_tensor, actions_real).mean()\n",
    "        actor_loss += self.args.action_l2 * (actions_real / self.env_params['action_max']).pow(2).mean()\n",
    "        # start to update the network\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        sync_grads(self.actor_network)\n",
    "        self.actor_optim.step()\n",
    "        # update the critic_network\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        sync_grads(self.critic_network)\n",
    "        self.critic_optim.step()\n",
    "\n",
    "    # do the evaluation\n",
    "    def _eval_agent(self):\n",
    "        total_success_rate = []\n",
    "        for _ in range(self.args.n_test_rollouts):\n",
    "            per_success_rate = []\n",
    "            observation = self.env.reset()\n",
    "            obs = observation['observation']\n",
    "            g = observation['desired_goal']\n",
    "            for _ in range(self.env_params['max_timesteps']):\n",
    "                with torch.no_grad():\n",
    "                    input_tensor = self._preproc_inputs(obs, g)\n",
    "                    pi = self.actor_network(input_tensor)\n",
    "                    # convert the actions\n",
    "                    actions = pi.detach().cpu().numpy().squeeze()\n",
    "                observation_new, _, _, info = self.env.step(actions)\n",
    "                obs = observation_new['observation']\n",
    "                g = observation_new['desired_goal']\n",
    "                per_success_rate.append(info['is_success'])\n",
    "            total_success_rate.append(per_success_rate)\n",
    "        total_success_rate = np.array(total_success_rate)\n",
    "        local_success_rate = np.mean(total_success_rate[:, -1])\n",
    "        global_success_rate = MPI.COMM_WORLD.allreduce(local_success_rate, op=MPI.SUM)\n",
    "        return global_success_rate / MPI.COMM_WORLD.Get_size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e6664",
   "metadata": {},
   "source": [
    "### Define environmental parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d33af9ba-1943-43b6-aa40-26ac1c0e1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_params(env):\n",
    "    obs = env.reset()\n",
    "    # close the environment\n",
    "    params = {'obs': obs['observation'].shape[0],\n",
    "            'goal': obs['desired_goal'].shape[0],\n",
    "            'action': env.action_space.shape[0],\n",
    "            'action_max': env.action_space.high[0],\n",
    "            }\n",
    "    params['max_timesteps'] = env._max_episode_steps\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176d2d5",
   "metadata": {},
   "source": [
    "### Parse arguments and run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2d10c28-4e5a-48ec-aab2-e0b04352f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch(args, path=None):\n",
    "    # Save path\n",
    "    save_path = os.path.join(path,f'{args.env_name}.wts')\n",
    "    # create the ddpg_agent\n",
    "    env = gym.make(args.env_name)\n",
    "    # get the environment parameters\n",
    "    env_params = get_env_params(env)\n",
    "    # create the ddpg agent to interact with the environment \n",
    "    ddpg_trainer = ddpg_agent(args, env, env_params)\n",
    "    agent_weights, onorm, gnorm = ddpg_trainer.learn()\n",
    "    # Save weights here\n",
    "    # torch.save(agent_weights.state_dict(), save_path)\n",
    "    # Save extra essential pieces\n",
    "    torch.save([onorm.mean, onorm.std, gnorm.mean, gnorm.std, agent_weights.state_dict()], save_path)\n",
    "    return agent_weights, onorm, gnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124b75d",
   "metadata": {},
   "source": [
    "### TODO: Cleanup the way code is launched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f3a8055-9301-4e74-b6ab-49e3703e0965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'obs': 25, 'goal': 3, 'action': 4, 'action_max': 1.0, 'max_timesteps': 50}\n"
     ]
    }
   ],
   "source": [
    "args_to_parse = '--env-name FetchSlide-v1 --n-epochs=200'\n",
    "args = get_args(args_to_parse)\n",
    "out = launch(args, fullPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fadb2952-c329-4ad5-b882-9784a43fa5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click a button to evaluate a policy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba6fa46ea5d41d994700722a333bc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='push', style=ButtonStyle()), Button(description='reach', style=ButtonStyle(â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n",
      "Creating window glfw\n",
      "Creating window glfw\n",
      "Creating window glfw\n",
      "Creating window glfw\n",
      "Creating window glfw\n",
      "Creating window glfw\n",
      "Creating window glfw\n",
      "Creating window glfw\n",
      "Creating window glfw\n"
     ]
    }
   ],
   "source": [
    "#  Jupyter UI - Trigger policies for individual agents\n",
    "def button_callback(button):\n",
    "    for b in buttons:\n",
    "        b.disabled = True\n",
    "\n",
    "    env = envs[button.description]['model']\n",
    "    final_policy = None\n",
    "    try:\n",
    "        env_name = envs[button.description]['model'].env.spec.id\n",
    "        env_params = get_env_params(env)\n",
    "        model = torch.load(f'{env_name}.wts')\n",
    "        model_weights = model[4]\n",
    "        actor_model = actor(env_params)\n",
    "        actor_model.load_state_dict(model_weights)\n",
    "        # Create policy using function\n",
    "        def policy(state):\n",
    "            # reset the environment\n",
    "            obs = state['observation']\n",
    "            g = state['desired_goal']\n",
    "            o_norm = normalizer(size=10, default_clip_range=200)\n",
    "            o_norm.mean = model[0]\n",
    "            o_norm.std = model[1]\n",
    "            g_norm = normalizer(size=3, default_clip_range=200)\n",
    "            g_norm.mean = model[2]\n",
    "            g_norm.std = model[3]\n",
    "            obs_norm = o_norm.normalize(obs)\n",
    "            g_norm = g_norm.normalize(g)\n",
    "            # concatenate the stuffs\n",
    "            input_tensor = np.concatenate([obs_norm, g_norm])\n",
    "            input_tensor = torch.tensor(input_tensor, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                pi = actor_model(input_tensor)\n",
    "            action = pi.cpu().numpy().squeeze()\n",
    "            # add the gaussian\n",
    "            action = np.clip(action, -1, 1)\n",
    "            return action\n",
    "        final_policy = policy\n",
    "    except Exception as e:\n",
    "        print('Could not create policy - running random policy')\n",
    "        print(str(e))\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "    render(env, final_policy)\n",
    "    env.close()\n",
    "        \n",
    "    for b in buttons:\n",
    "        b.disabled = False\n",
    "\n",
    "buttons = []\n",
    "for env_id in envs.keys():\n",
    "    button = widgets.Button(description=env_id)\n",
    "    button.on_click(button_callback)\n",
    "    buttons.append(button)\n",
    "\n",
    "print('Click a button to evaluate a policy')\n",
    "b = widgets.HBox(buttons)\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6ccc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
