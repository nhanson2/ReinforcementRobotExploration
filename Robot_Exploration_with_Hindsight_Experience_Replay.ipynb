{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da504c5",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Final Project: Exploration Strategies for Robotic Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5306d8",
   "metadata": {},
   "source": [
    "### Import helper libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9b0cc99-0fb3-41eb-8385-43664701355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important trick to keep JupyterLab from crashing\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import numpy as np\n",
    "import gym\n",
    "import sys\n",
    "import traceback\n",
    "from arguments import get_args\n",
    "import random\n",
    "import torch\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "from mpi4py import MPI\n",
    "from mpi_utils.mpi_utils import sync_networks, sync_grads\n",
    "from rl_modules.replay_buffer import replay_buffer\n",
    "from rl_modules.models import actor, critic, noisy_actor, noisy_critic\n",
    "from mpi_utils.normalizer import normalizer\n",
    "from her_modules.her import her_sampler\n",
    "import copy\n",
    "import math\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import more_itertools as mitt\n",
    "import pygame\n",
    "import glfw\n",
    "from math import floor\n",
    "from tiling import IHT\n",
    "from pathlib import Path\n",
    "from schedule import ExponentialSchedule, OUSchedule\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = [36, 4]\n",
    "fullPath = str(Path('.').absolute())\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['IN_MPI'] = '1'\n",
    "\n",
    "### Create environments and place to store model weights\n",
    "envs = {\n",
    "    'push': {\n",
    "        'model': gym.make('FetchPush-v1'),\n",
    "        'weights': None\n",
    "    },\n",
    "    'reach': {\n",
    "        'model': gym.make('FetchReach-v1'),\n",
    "        'weights': None\n",
    "    },\n",
    "    'slide': {\n",
    "        'model': gym.make('FetchSlide-v1'),\n",
    "        'weights': None\n",
    "    },\n",
    "    'pick': {\n",
    "        'model': gym.make('FetchPickAndPlace-v1'),\n",
    "        'weights': None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3954211",
   "metadata": {},
   "source": [
    "### Graphically Render Policy\n",
    "\n",
    "Using Mujoco backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c8f5f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env, policy=None):\n",
    "    \"\"\"Graphically render an episode using the given policy\n",
    "\n",
    "    :param env:  Gym environment\n",
    "    :param policy:  function which maps state to action.  If None, the random\n",
    "                    policy is used.\n",
    "    \"\"\"\n",
    "    glfw.init()\n",
    "    if policy is None:\n",
    "\n",
    "        def policy(state):\n",
    "            return env.action_space.sample()\n",
    "\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    i = 0\n",
    "    while i < 3000:\n",
    "        action = policy(state)\n",
    "        state, _, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "        i += 1\n",
    "            \n",
    "    env.close()\n",
    "    glfw.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736dc67",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de59111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_og(o, g):\n",
    "    o = np.clip(o, -200, 200)\n",
    "    g = np.clip(g, -200, 200)\n",
    "    return o, g\n",
    "\n",
    "# pre_process the inputs\n",
    "def preproc_inputs(obs, g):\n",
    "    obs_norm = self.o_norm.normalize(obs)\n",
    "    g_norm = self.g_norm.normalize(g)\n",
    "    # concatenate the stuffs\n",
    "    inputs = np.concatenate([obs_norm, g_norm])\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b8072a",
   "metadata": {},
   "source": [
    "### Define Structure for Deep Deterministic Policy Gradient (DDPG) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d5496d8a-67e0-45af-9871-e859ecbca084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ddpg with HER (MPI-version)\n",
    "\n",
    "\"\"\"\n",
    "class ddpg_agent:\n",
    "    def __init__(self, args, env, env_params):\n",
    "        self.args = args\n",
    "        self.env = env\n",
    "        self.env_params = env_params\n",
    "        # create the network\n",
    "        self.actor_network = noisy_actor(env_params) if bool(self.args.noisy) else actor(env_params)\n",
    "        self.critic_network = noisy_critic(env_params) if bool(self.args.noisy) else critic(env_params) \n",
    "        # sync the networks across the cpus\n",
    "        sync_networks(self.actor_network)\n",
    "        sync_networks(self.critic_network)\n",
    "        # build up the target network\n",
    "        self.actor_target_network = actor(env_params)\n",
    "        self.critic_target_network = critic(env_params)\n",
    "        # load the weights into the target networks\n",
    "        self.actor_target_network.load_state_dict(self.actor_network.state_dict())\n",
    "        self.critic_target_network.load_state_dict(self.critic_network.state_dict())\n",
    "        # if use gpu\n",
    "        if self.args.cuda:\n",
    "            self.actor_network.cuda()\n",
    "            self.critic_network.cuda()\n",
    "            self.actor_target_network.cuda()\n",
    "            self.critic_target_network.cuda()\n",
    "        # create the optimizer\n",
    "        self.actor_optim = torch.optim.Adam(self.actor_network.parameters(), lr=self.args.lr_actor)\n",
    "        self.critic_optim = torch.optim.Adam(self.critic_network.parameters(), lr=self.args.lr_critic)\n",
    "        # her sampler\n",
    "        self.her_module = her_sampler(self.args.replay_strategy, self.args.replay_k, self.env.compute_reward)\n",
    "        # create the replay buffer\n",
    "        self.buffer = replay_buffer(self.env_params, self.args.buffer_size, self.her_module.sample_her_transitions)\n",
    "        # create the normalizer\n",
    "        self.o_norm = normalizer(size=env_params['obs'], default_clip_range=self.args.clip_range)\n",
    "        self.g_norm = normalizer(size=env_params['goal'], default_clip_range=self.args.clip_range)\n",
    "        ########### Setup exploration parameters ########\n",
    "        # Create exploration parameters\n",
    "        self.epsilon = self.args.epsilon\n",
    "        # Create e-greedy schedule\n",
    "        self.e_schedule = ExponentialSchedule(1.0,0.01,self.args.n_cycles)\n",
    "        self.ou_schedule = OUSchedule(self.args.n_cycles,4)\n",
    "        # Create store for count based methods\n",
    "        self.num_tiles = 10\n",
    "        self.max_size = 1000\n",
    "        self.iht = IHT(self.max_size)\n",
    "        self.weights_fq = np.zeros(self.max_size)\n",
    "        self.weights_ucb = np.zeros(self.max_size)\n",
    "        # Create strategy store\n",
    "        self.strategies = {\n",
    "        'no_explore': self.no_explore,\n",
    "        'standard': self.standard_explore,\n",
    "        'e_greedy': self.e_greedy,\n",
    "        'e_greedy_decay': self.e_greedy_decay,\n",
    "        'ucb': self.ucb,\n",
    "        'ou': self.ornstein_uhlenbeck,\n",
    "        'count': self.frequency_explore\n",
    "        }\n",
    "        # Select the exploration strategy\n",
    "        self.explore_strategy = self.strategies[self.args.strategy]\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        train the network\n",
    "\n",
    "        \"\"\"\n",
    "        # start to collect samples\n",
    "        pbar = tqdm.notebook.trange(self.args.n_epochs)\n",
    "        for epoch in pbar:\n",
    "            # Reset process noise\n",
    "            self.ou_schedule.reset()\n",
    "            # Reset count based method stores\n",
    "            self.iht = IHT(self.max_size)\n",
    "            self.weights_fq = np.zeros(self.max_size)\n",
    "            self.weights_ucb = np.zeros(self.max_size)\n",
    "            for cycle_num in range(self.args.n_cycles):\n",
    "                mb_obs, mb_ag, mb_g, mb_actions = [], [], [], []\n",
    "                for _ in range(self.args.num_rollouts_per_mpi):\n",
    "                    # reset the rollouts\n",
    "                    ep_obs, ep_ag, ep_g, ep_actions = [], [], [], []\n",
    "                    # reset the environment\n",
    "                    observation = self.env.reset()\n",
    "                    obs = observation['observation']\n",
    "                    ag = observation['achieved_goal']\n",
    "                    g = observation['desired_goal']\n",
    "                    # start to collect samples\n",
    "                    for t in range(self.env_params['max_timesteps']):\n",
    "                        with torch.no_grad():\n",
    "                            input_tensor = self._preproc_inputs(obs, g)\n",
    "                            pi = self.actor_network(input_tensor)\n",
    "                            action = self._select_actions(pi, input_tensor, cycle_num)\n",
    "                        # feed the actions into the environment\n",
    "                        observation_new, _, _, info = self.env.step(action)\n",
    "                        obs_new = observation_new['observation']\n",
    "                        ag_new = observation_new['achieved_goal']\n",
    "                        # append rollouts\n",
    "                        ep_obs.append(obs.copy())\n",
    "                        ep_ag.append(ag.copy())\n",
    "                        ep_g.append(g.copy())\n",
    "                        ep_actions.append(action.copy())\n",
    "                        # re-assign the observation\n",
    "                        obs = obs_new\n",
    "                        ag = ag_new\n",
    "                    ep_obs.append(obs.copy())\n",
    "                    ep_ag.append(ag.copy())\n",
    "                    mb_obs.append(ep_obs)\n",
    "                    mb_ag.append(ep_ag)\n",
    "                    mb_g.append(ep_g)\n",
    "                    mb_actions.append(ep_actions)\n",
    "                # convert them into arrays\n",
    "                mb_obs = np.array(mb_obs)\n",
    "                mb_ag = np.array(mb_ag)\n",
    "                mb_g = np.array(mb_g)\n",
    "                mb_actions = np.array(mb_actions)\n",
    "                # store the episodes\n",
    "                self.buffer.store_episode([mb_obs, mb_ag, mb_g, mb_actions])\n",
    "                self._update_normalizer([mb_obs, mb_ag, mb_g, mb_actions])\n",
    "                for _ in range(self.args.n_batches):\n",
    "                    # train the network\n",
    "                    self._update_network()\n",
    "                # soft update\n",
    "                self._soft_update_target_network(self.actor_target_network, self.actor_network)\n",
    "                self._soft_update_target_network(self.critic_target_network, self.critic_network)\n",
    "            # start to do the evaluation\n",
    "            success_rate = self._eval_agent()\n",
    "            if MPI.COMM_WORLD.Get_rank() == 0:\n",
    "                print('[{}] epoch is: {}, eval success rate is: {:.3f}'.format(datetime.now(), epoch, success_rate))\n",
    "\n",
    "        # Return the network weights\n",
    "        #torch.save([self.o_norm.mean, self.o_norm.std, self.g_norm.mean, self.g_norm.std, self.actor_network.state_dict()], \\\n",
    "        #    self.model_path + '/model.pt')\n",
    "        return self.actor_network, self.o_norm, self.g_norm\n",
    "    \n",
    "\n",
    "    def get_active_tiles(self, state, action):\n",
    "        active_tiles = self.tiles(self.iht, self.num_tiles, np.concatenate((state,action)))\n",
    "        return active_tiles\n",
    "\n",
    "    def hash_coords(self, coordinates, m, read_only=False):\n",
    "        if isinstance(m, IHT): return m.get_index(tuple(coordinates), read_only)\n",
    "        if isinstance(m, int): return hash(tuple(coordinates)) % m\n",
    "        if m is None: return coordinates\n",
    "\n",
    "    def tiles(self, iht_or_size, num_tilings, floats, ints=None, read_only=False):\n",
    "        \"\"\"returns num-tilings tile indices corresponding to the floats and ints\"\"\"\n",
    "        if ints is None:\n",
    "            ints = []\n",
    "        qfloats = [floor(f * num_tilings) for f in floats]\n",
    "        tiles = []\n",
    "        for tiling in range(num_tilings):\n",
    "            tilingX2 = tiling * 2\n",
    "            coords = [tiling]\n",
    "            b = tiling\n",
    "            for q in qfloats:\n",
    "                coords.append((q + b) // num_tilings)\n",
    "                b += tilingX2\n",
    "            coords.extend(ints)\n",
    "            tiles.append(self.hash_coords(coords, iht_or_size, read_only))\n",
    "        return tiles\n",
    "\n",
    "    # no exploration, returns the action\n",
    "    def no_explore(self, action, state, step):\n",
    "        return action\n",
    "        \n",
    "    # run basic e-greedy exploration\n",
    "    def e_greedy(self, action, state, step):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Select a random action state\n",
    "            action = np.random.uniform(low=-self.env_params['action_max'], high=self.env_params['action_max'], \\\n",
    "                                            size=self.env_params['action'])\n",
    "        return action\n",
    "        \n",
    "    # Use decaying e-greedy strategy\n",
    "    def e_greedy_decay(self, action, state, step):\n",
    "        if np.random.random() < self.e_schedule.value(step):\n",
    "            # Select a random action state\n",
    "            action = np.random.uniform(low=-self.env_params['action_max'], high=self.env_params['action_max'], \\\n",
    "                                            size=self.env_params['action'])\n",
    "        return action\n",
    "    \n",
    "    # Use upper confidence bound\n",
    "    def ucb(self, action, state, step):\n",
    "        active = self.get_active_tiles(np.random.normal(state.numpy().flatten()),np.random.normal(action))\n",
    "        self.weights_ucb[active] += 1\n",
    "        additive = np.sqrt(np.log(np.sum(self.weights_ucb[active]))/np.sum(self.weights_ucb))\n",
    "        return action + np.random.normal(loc=0.0,scale=additive,size=4)\n",
    "    \n",
    "    # Select action as a function of how frequently they have occured\n",
    "    def frequency_explore(self, action, state, step):\n",
    "        beta = 0.5\n",
    "        active = self.get_active_tiles(np.random.normal(state.numpy().flatten()),np.random.normal(action))\n",
    "        self.weights_fq[active] += 1\n",
    "        additive = beta/np.sqrt(np.sum(self.weights_fq[active]))\n",
    "        return action + np.random.normal(loc=0.0,scale=additive,size=4)\n",
    "\n",
    "    # White noise based exploration\n",
    "    def standard_explore(self, action, state, step):\n",
    "        # random actions...\n",
    "        random_actions = np.random.uniform(low=-self.env_params['action_max'], high=self.env_params['action_max'], \\\n",
    "                                            size=self.env_params['action'])\n",
    "        # choose if use the random actions\n",
    "        return action + np.random.binomial(1, self.args.random_eps, 1)[0] * (random_actions - action)\n",
    "    \n",
    "    # Explore using OU process\n",
    "    def ornstein_uhlenbeck(self, action, state, step):\n",
    "        return action + self.ou_schedule.value(step)\n",
    "    \n",
    "    # pre_process the inputs\n",
    "    def _preproc_inputs(self, obs, g):\n",
    "        obs_norm = self.o_norm.normalize(obs)\n",
    "        g_norm = self.g_norm.normalize(g)\n",
    "        # concatenate the stuffs\n",
    "        inputs = np.concatenate([obs_norm, g_norm])\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)\n",
    "        if self.args.cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        return inputs\n",
    "    \n",
    "    # this function will choose action for the agent and do the exploration\n",
    "    def _select_actions(self, pi, state, step):\n",
    "        action = pi.cpu().numpy().squeeze()\n",
    "        # add the gaussian\n",
    "        #action += self.args.noise_eps * self.env_params['action_max'] * np.random.randn(*action.shape)\n",
    "        #action = np.clip(action, -self.env_params['action_max'], self.env_params['action_max'])\n",
    "        action = self.explore_strategy(action,state,step)\n",
    "        action = np.clip(action, -self.env_params['action_max'], self.env_params['action_max'])\n",
    "        return action\n",
    "\n",
    "    # update the normalizer\n",
    "    def _update_normalizer(self, episode_batch):\n",
    "        mb_obs, mb_ag, mb_g, mb_actions = episode_batch\n",
    "        mb_obs_next = mb_obs[:, 1:, :]\n",
    "        mb_ag_next = mb_ag[:, 1:, :]\n",
    "        # get the number of normalization transitions\n",
    "        num_transitions = mb_actions.shape[1]\n",
    "        # create the new buffer to store them\n",
    "        buffer_temp = {'obs': mb_obs, \n",
    "                       'ag': mb_ag,\n",
    "                       'g': mb_g, \n",
    "                       'actions': mb_actions, \n",
    "                       'obs_next': mb_obs_next,\n",
    "                       'ag_next': mb_ag_next,\n",
    "                       }\n",
    "        transitions = self.her_module.sample_her_transitions(buffer_temp, num_transitions)\n",
    "        obs, g = transitions['obs'], transitions['g']\n",
    "        # pre process the obs and g\n",
    "        transitions['obs'], transitions['g'] = preproc_og(obs, g)\n",
    "        # update\n",
    "        self.o_norm.update(transitions['obs'])\n",
    "        self.g_norm.update(transitions['g'])\n",
    "        # recompute the stats\n",
    "        self.o_norm.recompute_stats()\n",
    "        self.g_norm.recompute_stats()\n",
    "\n",
    "    # soft update\n",
    "    def _soft_update_target_network(self, target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_((1 - self.args.polyak) * param.data + self.args.polyak * target_param.data)\n",
    "\n",
    "    # update the network\n",
    "    def _update_network(self):\n",
    "        # sample the episodes\n",
    "        transitions = self.buffer.sample(self.args.batch_size)\n",
    "        # pre-process the observation and goal\n",
    "        o, o_next, g = transitions['obs'], transitions['obs_next'], transitions['g']\n",
    "        transitions['obs'], transitions['g'] = preproc_og(o, g)\n",
    "        transitions['obs_next'], transitions['g_next'] = preproc_og(o_next, g)\n",
    "        # start to do the update\n",
    "        obs_norm = self.o_norm.normalize(transitions['obs'])\n",
    "        g_norm = self.g_norm.normalize(transitions['g'])\n",
    "        inputs_norm = np.concatenate([obs_norm, g_norm], axis=1)\n",
    "        obs_next_norm = self.o_norm.normalize(transitions['obs_next'])\n",
    "        g_next_norm = self.g_norm.normalize(transitions['g_next'])\n",
    "        inputs_next_norm = np.concatenate([obs_next_norm, g_next_norm], axis=1)\n",
    "        # transfer them into the tensor\n",
    "        inputs_norm_tensor = torch.tensor(inputs_norm, dtype=torch.float32)\n",
    "        inputs_next_norm_tensor = torch.tensor(inputs_next_norm, dtype=torch.float32)\n",
    "        actions_tensor = torch.tensor(transitions['actions'], dtype=torch.float32)\n",
    "        r_tensor = torch.tensor(transitions['r'], dtype=torch.float32) \n",
    "        if self.args.cuda:\n",
    "            inputs_norm_tensor = inputs_norm_tensor.cuda()\n",
    "            inputs_next_norm_tensor = inputs_next_norm_tensor.cuda()\n",
    "            actions_tensor = actions_tensor.cuda()\n",
    "            r_tensor = r_tensor.cuda()\n",
    "        # calculate the target Q value function\n",
    "        with torch.no_grad():\n",
    "            # do the normalization\n",
    "            # concatenate the stuffs\n",
    "            actions_next = self.actor_target_network(inputs_next_norm_tensor)\n",
    "            q_next_value = self.critic_target_network(inputs_next_norm_tensor, actions_next)\n",
    "            q_next_value = q_next_value.detach()\n",
    "            target_q_value = r_tensor + self.args.gamma * q_next_value\n",
    "            target_q_value = target_q_value.detach()\n",
    "            # clip the q value\n",
    "            clip_return = 1 / (1 - self.args.gamma)\n",
    "            target_q_value = torch.clamp(target_q_value, -clip_return, 0)\n",
    "        # the q loss\n",
    "        real_q_value = self.critic_network(inputs_norm_tensor, actions_tensor)\n",
    "        critic_loss = (target_q_value - real_q_value).pow(2).mean()\n",
    "        # the actor loss\n",
    "        actions_real = self.actor_network(inputs_norm_tensor)\n",
    "        actor_loss = -self.critic_network(inputs_norm_tensor, actions_real).mean()\n",
    "        actor_loss += self.args.action_l2 * (actions_real / self.env_params['action_max']).pow(2).mean()\n",
    "        # start to update the network\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        sync_grads(self.actor_network)\n",
    "        self.actor_optim.step()\n",
    "        # update the critic_network\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        sync_grads(self.critic_network)\n",
    "        self.critic_optim.step()\n",
    "\n",
    "    # do the evaluation\n",
    "    def _eval_agent(self):\n",
    "        total_success_rate = []\n",
    "        for _ in range(self.args.n_test_rollouts):\n",
    "            per_success_rate = []\n",
    "            observation = self.env.reset()\n",
    "            obs = observation['observation']\n",
    "            g = observation['desired_goal']\n",
    "            for _ in range(self.env_params['max_timesteps']):\n",
    "                with torch.no_grad():\n",
    "                    input_tensor = self._preproc_inputs(obs, g)\n",
    "                    pi = self.actor_network(input_tensor)\n",
    "                    # convert the actions\n",
    "                    actions = pi.detach().cpu().numpy().squeeze()\n",
    "                observation_new, _, _, info = self.env.step(actions)\n",
    "                obs = observation_new['observation']\n",
    "                g = observation_new['desired_goal']\n",
    "                per_success_rate.append(info['is_success'])\n",
    "            total_success_rate.append(per_success_rate)\n",
    "        total_success_rate = np.array(total_success_rate)\n",
    "        local_success_rate = np.mean(total_success_rate[:, -1])\n",
    "        global_success_rate = MPI.COMM_WORLD.allreduce(local_success_rate, op=MPI.SUM)\n",
    "        return global_success_rate / MPI.COMM_WORLD.Get_size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e6664",
   "metadata": {},
   "source": [
    "### Define environmental parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d33af9ba-1943-43b6-aa40-26ac1c0e1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_params(env):\n",
    "    obs = env.reset()\n",
    "    # close the environment\n",
    "    params = {'obs': obs['observation'].shape[0],\n",
    "            'goal': obs['desired_goal'].shape[0],\n",
    "            'action': env.action_space.shape[0],\n",
    "            'action_max': env.action_space.high[0],\n",
    "            }\n",
    "    params['max_timesteps'] = env._max_episode_steps\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176d2d5",
   "metadata": {},
   "source": [
    "### Parse arguments and run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2d10c28-4e5a-48ec-aab2-e0b04352f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch(args, path=None):\n",
    "    # Save path\n",
    "    save_path = os.path.join(path,f'{args.env_name}.wts')\n",
    "    # create the ddpg_agent\n",
    "    env = gym.make(args.env_name)\n",
    "    # get the environment parameters\n",
    "    env_params = get_env_params(env)\n",
    "    # create the ddpg agent to interact with the environment \n",
    "    ddpg_trainer = ddpg_agent(args, env, env_params)\n",
    "    agent_weights, onorm, gnorm = ddpg_trainer.learn()\n",
    "    # Save weights here\n",
    "    # torch.save(agent_weights.state_dict(), save_path)\n",
    "    # Save extra essential pieces\n",
    "    torch.save([onorm.mean, onorm.std, gnorm.mean, gnorm.std, agent_weights.state_dict()], save_path)\n",
    "    return agent_weights, onorm, gnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124b75d",
   "metadata": {},
   "source": [
    "### TODO: Cleanup the way code is launched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7f3a8055-9301-4e74-b6ab-49e3703e0965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b65f7f2776d429e92c5cf258075592b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IHT full, starting to allow collisions\n",
      "[2021-12-06 18:10:46.768710] epoch is: 0, eval success rate is: 0.500\n",
      "IHT full, starting to allow collisions\n",
      "[2021-12-06 18:11:04.378719] epoch is: 1, eval success rate is: 1.000\n",
      "IHT full, starting to allow collisions\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2863069/2121939426.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0margs_to_parse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'--env-name FetchReach-v1 --n-epochs=50 --strategy=ucb --noisy=1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_to_parse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2863069/2349615471.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(args, path)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# create the ddpg agent to interact with the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mddpg_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0magent_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Save weights here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# torch.save(agent_weights.state_dict(), save_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2863069/1498204811.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m                             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                         \u001b[0;31m# feed the actions into the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                         \u001b[0mobservation_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                         \u001b[0mobs_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'observation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                         \u001b[0mag_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'achieved_goal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         ), \"Cannot call env.step() before calling reset()\"\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gym/envs/robotics/robot_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gym/envs/robotics/fetch_env.py\u001b[0m in \u001b[0;36m_get_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnsubsteps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mgrip_velp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_site_xvelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"robot0:grip\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mrobot_qpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrobot_qvel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_object\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mobject_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_site_xpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"object0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gym/envs/robotics/utils.py\u001b[0m in \u001b[0;36mrobot_get_obs\u001b[0;34m(sim)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint_names\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"robot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         return (\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_joint_qpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_joint_qvel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gym/envs/robotics/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint_names\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"robot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         return (\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_joint_qpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_joint_qvel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args_to_parse = '--env-name FetchReach-v1 --n-epochs=50 --strategy=ucb --noisy=1'\n",
    "args = get_args(args_to_parse)\n",
    "out = launch(args, fullPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb2952-c329-4ad5-b882-9784a43fa5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Jupyter UI - Trigger policies for individual agents\n",
    "def button_callback(button):\n",
    "    for b in buttons:\n",
    "        b.disabled = True\n",
    "\n",
    "    env = envs[button.description]['model']\n",
    "    final_policy = None\n",
    "    try:\n",
    "        env_name = envs[button.description]['model'].env.spec.id\n",
    "        env_params = get_env_params(env)\n",
    "        model = torch.load(f'{env_name}.wts')\n",
    "        model_weights = model[4]\n",
    "        actor_model = actor(env_params)\n",
    "        actor_model.load_state_dict(model_weights)\n",
    "        # Create policy using function\n",
    "        def policy(state):\n",
    "            # reset the environment\n",
    "            obs = state['observation']\n",
    "            g = state['desired_goal']\n",
    "            o_norm = normalizer(size=10, default_clip_range=200)\n",
    "            o_norm.mean = model[0]\n",
    "            o_norm.std = model[1]\n",
    "            g_norm = normalizer(size=3, default_clip_range=200)\n",
    "            g_norm.mean = model[2]\n",
    "            g_norm.std = model[3]\n",
    "            obs_norm = o_norm.normalize(obs)\n",
    "            g_norm = g_norm.normalize(g)\n",
    "            # concatenate the stuffs\n",
    "            input_tensor = np.concatenate([obs_norm, g_norm])\n",
    "            input_tensor = torch.tensor(input_tensor, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                pi = actor_model(input_tensor)\n",
    "            action = pi.cpu().numpy().squeeze()\n",
    "            # add the gaussian\n",
    "            action = np.clip(action, -1, 1)\n",
    "            return action\n",
    "        final_policy = policy\n",
    "    except Exception as e:\n",
    "        print('Could not create policy - running random policy')\n",
    "        print(str(e))\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "    render(env, final_policy)\n",
    "    env.close()\n",
    "        \n",
    "    for b in buttons:\n",
    "        b.disabled = False\n",
    "\n",
    "buttons = []\n",
    "for env_id in envs.keys():\n",
    "    button = widgets.Button(description=env_id)\n",
    "    button.on_click(button_callback)\n",
    "    buttons.append(button)\n",
    "\n",
    "print('Click a button to evaluate a policy')\n",
    "b = widgets.HBox(buttons)\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7dcb01-d13a-4436-9de7-48331bccd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tiles = 10\n",
    "max_size = 1000\n",
    "iht = IHT(max_size)\n",
    "\n",
    "def get_active_tiles(state, action):\n",
    "    active_tiles = tiles(iht, num_tiles, np.concatenate((state,action)))\n",
    "    return active_tiles\n",
    "\n",
    "def hash_coords(coordinates, m, read_only=False):\n",
    "    if isinstance(m, IHT): return m.get_index(tuple(coordinates), read_only)\n",
    "    if isinstance(m, int): return hash(tuple(coordinates)) % m\n",
    "    if m is None: return coordinates\n",
    "\n",
    "def tiles(iht_or_size, num_tilings, floats, ints=None, read_only=False):\n",
    "    \"\"\"returns num-tilings tile indices corresponding to the floats and ints\"\"\"\n",
    "    if ints is None:\n",
    "        ints = []\n",
    "    qfloats = [floor(f * num_tilings) for f in floats]\n",
    "    tiles = []\n",
    "    for tiling in range(num_tilings):\n",
    "        tilingX2 = tiling * 2\n",
    "        coords = [tiling]\n",
    "        b = tiling\n",
    "        for q in qfloats:\n",
    "            coords.append((q + b) // num_tilings)\n",
    "            b += tilingX2\n",
    "        coords.extend(ints)\n",
    "        tiles.append(hash_coords(coords, iht_or_size, read_only))\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07608a-cd1f-4905-aba7-2fa42b320fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = envs['push']['model'].reset()\n",
    "weights = np.zeros(max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e16f09-2248-4131-8eaf-633cf93a6f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "ucb = []\n",
    "beta = 0.5\n",
    "for x in range(10000):\n",
    "    active = get_active_tiles(np.random.normal(size=25),np.random.normal(size=3))\n",
    "    weights[active] += 1\n",
    "    additive = beta/np.sqrt(np.sum(weights[active]))\n",
    "    t.append(additive)\n",
    "    ucb.append(np.sqrt(np.log(np.sum(weights[active]))/np.sum(weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a37b43-545f-4d5d-b487-c43bc5a66082",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t,label='count')\n",
    "plt.plot(ucb,label='ucb')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
